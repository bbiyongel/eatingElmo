{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/anaconda3/envs/ner-anago/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import anago"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/envs/ner-anago/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /anaconda3/envs/ner-anago/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /anaconda3/envs/ner-anago/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /anaconda3/envs/ner-anago/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2974: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /anaconda3/envs/ner-anago/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /anaconda3/envs/ner-anago/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /anaconda3/envs/ner-anago/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /anaconda3/envs/ner-anago/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /anaconda3/envs/ner-anago/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /anaconda3/envs/ner-anago/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /anaconda3/envs/ner-anago/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /anaconda3/envs/ner-anago/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 챗봇 4단계 모델 불러오기\n",
    "weights = './mecab_model_weights'\n",
    "params = './mecab_model_params'\n",
    "preprocessor = './mecab_model_preprocessor'\n",
    "model = anago.Sequence.load(weights, params, preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소 분석기 불러오기\n",
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "from konlpy.tag import Mecab\n",
    "mecab = Mecab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 챗봇 3단계 모델 불러오기\n",
    "import pickle\n",
    "with open('./classifier_tfidf_3.pickle', \"rb\") as f:\n",
    "    classifier_tfidf = pickle.load(f)\n",
    "with open(\"./mlb_3.pickle\", \"rb\") as f:\n",
    "    mlb = pickle.load(f)\n",
    "with open(\"./tfidf_vectorizer_3.pickle\", \"rb\") as f:\n",
    "    tfidf_vectorizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 챗봇 3단계 모델 실행에 필요한 함수 정의\n",
    "QUESTION_RE = re.compile('[^ ㄱ-ㅣ가-힣a-zA-Z]+')\n",
    "\n",
    "STOPWORDS = set(['은','는','이','가','하','아','것','들','의','그','수','한','나','같','그렇'\n",
    "                ,'문제','그리고','크','중','나오','지금','생각하','집','어떤','명','생각','이런'\n",
    "                ,'인','지','을','를','에','스러운','스러워','주','할','만','게','도','져','된','로','고','던','로운','면서'\n",
    "                ,'사실','이렇','점','싶','말','좀','식당','가게','집','음식점'\n",
    "                ,'는지','나요','해요','해','는가요','삼','게요','예','는가','습니까','죠','려고요','는지요','서요','였어요','겠'\n",
    "                ,'인가요','요' '라는','데','해서','세요','어요','을까요','건가요','겠죠','실래요','네요','으세요','지요','인데요'\n",
    "                ,'드려요','려구요','합니다'])\n",
    "\n",
    "def text_prepare(text):\n",
    "    \n",
    "    # 한글과 스페이스바를 제외한 다른 문자들은 모두 지운다.\n",
    "    text = QUESTION_RE.sub('', text) \n",
    "    \n",
    "    # mecab 토크나이저\n",
    "    mecab = Mecab()\n",
    "    \n",
    "    # mecab으로 text를 형태소 단위로 나누고 불용어를 지운다.\n",
    "    text = ' '.join(token for token in mecab.morphs(text) if token not in STOPWORDS)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 테스트용 질문 제작 (분석 잘된거/안된거, 각각 3개씩, 총 102개(=51*2))\n",
    "\n",
    "***추천 요청 질문 말뭉치*** \n",
    "- `음식메뉴` : train 메뉴리스트에 있는 것/없는 것 둘 다 사용해볼 것\n",
    "- `형용사`,`장소`,`어미` : 실제로 사용자들이 챗봇에게 물어볼만한 말투\n",
    "\n",
    "- 1) 음식메뉴 + 추천요청어미\n",
    "- 2) 음식메뉴 + 형용사 + 추천요청어미\n",
    "- 3) 음식메뉴 + 형용사 + 장소 + 추천요청어미\n",
    "- 4) 음식메뉴 + 장소 + 형용사 + 추천요청어미\n",
    "- 5) 형용사 + 추천요청어미\n",
    "- 6) 형용사 + 음식메뉴 + 추천요청어미\n",
    "- 7) 형용사 + 형용사 + 추천요청어미\n",
    "- 8) 형용사 + 음식메뉴 + 장소 + 추천요청어미\n",
    "- 9) 장소 + 추천요청어미\n",
    "- 10) 장소 + 음식메뉴 + 추천요청어미\n",
    "- 11) 장소 + 형용사 + 추천요청어미\n",
    "- 12) 장소 + 형용사 + 음식메뉴 + 추천요청어미\n",
    "\n",
    "***문의(장소,리뷰) 질문 말뭉치***\n",
    "- `가게명` : 골목식당 가게리스트 사용\n",
    "- `형용사`,`장소`,`어미` : 실제로 사용자들이 챗봇에게 물어볼만한 말투\n",
    "\n",
    "- 1) 가게명 + 장소문의어미\n",
    "- 2) 가게명 + 리뷰문의어미\n",
    "- 3) 가게명 + 형용사 + 리뷰문의어미\n",
    "- 4) 장소 + 가게명 + 리뷰문의어미\n",
    "- 5) 장소 + 가게명 + 형용사 + 리뷰문의어미"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 먼저 아래 셀에서 `text`를 바꿔가며 여러 질문을 테스트한다.\n",
    "2. 형용사/음식메뉴/장소/가게명의 분석이 잘 된 것은 `X_good`, 뭔가 하나라도 실패한 것은 `X_bad` list에 저장한다.\n",
    "    - 형용사 : `ADJ`, 음식메뉴 :  `FOD`, 장소 : `LOC`, 가게명 : `ORG`\n",
    "    \n",
    "    - `X_good`,`X_bad` list 안의 질문의 개수는 각각 51개씩, (더 많아도 상관 없음)\n",
    "3. `y_good`,`y_bad` list 안에 각각의 tag를 단다. \n",
    "    - (1) : recommendation/questionLOC/questionREV\n",
    "    - (2) : taste/sanitation/price/atmosphere/prefer\n",
    "    - (3) : korean/chinese/western/japanese/chicken/asian/etc\n",
    "    - 태그를 달 때 (1)은 반드시 포함해야 한다. (2),(3)은 포함할 수도 있고 포함하지 않을 수도 있다.\n",
    "    - (1),(2),(3) 각 태그 종류에서 1개 태그만 선택해야 한다. (korean,chinese를 동시에 달 수 없음)\n",
    "4. `X_good`, `X_bad` 각각에서 챗봇 4단계, 챗봇 3단계 모델을 실행하고 결과를 저장한다.\n",
    "5. `okt`/`mecab` 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'words': ['맛있는', '제육', '덮', '밥', '집', '알려줘'],\n",
       " 'entities': [{'text': '맛있는',\n",
       "   'type': 'ADJ',\n",
       "   'score': 1.0,\n",
       "   'beginOffset': 0,\n",
       "   'endOffset': 1},\n",
       "  {'text': '제육 덮 밥 집',\n",
       "   'type': 'FOD',\n",
       "   'score': 1.0,\n",
       "   'beginOffset': 1,\n",
       "   'endOffset': 5}]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. text를 바꿔가며 테스트\n",
    "text = '맛있는 제육덮밥 집 알려줘'\n",
    "text_prepared = ' '.join(word for word in mecab.morphs(text))\n",
    "model.analyze(text_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 1번에서 형용사/음식메뉴/장소/가게명의 분석이 잘 되는 질문은 X_good, 잘 안되면 X_bad에 담는다.\n",
    "X_good = [\n",
    "    '맛있는 제육덮밥 집 알려줘'\n",
    "    , '다음 질문'\n",
    "]\n",
    "X_bad = [\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': '맛있는',\n",
       "  'type': 'ADJ',\n",
       "  'score': 1.0,\n",
       "  'beginOffset': 0,\n",
       "  'endOffset': 1},\n",
       " {'text': '제육 덮 밥 집',\n",
       "  'type': 'FOD',\n",
       "  'score': 1.0,\n",
       "  'beginOffset': 1,\n",
       "  'endOffset': 5}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_anal['entities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. X_good과 X_good 각각에 태그 y_good, y_bad를 단다.\n",
    "y_good = [\n",
    "    \n",
    "]\n",
    "y_bad = [\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 예시\n",
    "X_chatbot = [\n",
    "    '맛있는 파스타집 추천해주세요'\n",
    "    , '종로에 분위기 좋은 순대국밥집 있나요?'\n",
    "    , '청결한 한식집 있어요?'\n",
    "    , '가성비 좋은 짬뽕 집 알려주세요'\n",
    "    , '줄 서서 기다리는 마라탕 집 있을까요'\n",
    "    , '보쌈집 위치 알려주십시오'\n",
    "    , '언니네횟집 주소좀요'\n",
    "    , '언니네횟집 후기 좀 알려주세요'\n",
    "    , '언니네횟집 사람들 반응점'\n",
    "]\n",
    "y_chatbot = [\n",
    "    ['recommendation','taste','western']\n",
    "    ,['recommendation','atmosphere','korean']\n",
    "    ,['recommendation','sanitation','korean']\n",
    "    ,['recommendation','price','chinese']\n",
    "    ,['recommendation','prefer','chinese']\n",
    "    ,['questionLOC']\n",
    "    ,['questionLOC']\n",
    "    ,['questionREV']\n",
    "    ,['questionREV']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 챗봇 3단계 모델 실행결과\n",
    "\n",
    "# SENTENCE 데이터 전처리\n",
    "X_good_prepared = [text_prepare(x) for x in X_good]\n",
    "X_bad_prepared = [text_prepare(x) for x in X_bad]\n",
    "\n",
    "# tf-idf 벡터화\n",
    "X_good_tfidf = tfidf_vectorizer.transform(X_good)\n",
    "X_bad_tfidf = tfidf_vectorizer.transform(X_bad)\n",
    "\n",
    "# tag 데이터 전처리\n",
    "y_good = mlb.transform(y_good)\n",
    "y_bad = mlb.transform(y_bad)\n",
    "\n",
    "# tag 예측\n",
    "y_good_predicted_labels_tfidf = classifier_tfidf.predict(X_good_tfidf)\n",
    "y_good_predicted_scores_tfidf = classifier_tfidf.decision_function(X_good_tfidf)\n",
    "y_bad_predicted_labels_tfidf = classifier_tfidf.predict(X_bad_tfidf)\n",
    "y_bad_predicted_scores_tfidf = classifier_tfidf.decision_function(X_bad_tfidf)\n",
    "\n",
    "# 예측된 tag를 텍스트로 변환\n",
    "y_good_pred_inversed = mlb.inverse_transform(y_good_predicted_labels_tfidf)\n",
    "y_good_inversed = mlb.inverse_transform(y_chatbot)\n",
    "y_bad_pred_inversed = mlb.inverse_transform(y_bad_predicted_labels_tfidf)\n",
    "y_bad_inversed = mlb.inverse_transform(y_bad)\n",
    "\n",
    "for i in range(0,len(y_good)):\n",
    "    print('SENTENCE:\\t{}\\n정답:\\t{}\\n예측:\\t{}\\n\\n'.format(\n",
    "        X_good[i],\n",
    "        y_good_inversed[i],\n",
    "        y_good_pred_inversed[i]\n",
    "    ))\n",
    "for i in range(0,len(y_bad)):\n",
    "    print('SENTENCE:\\t{}\\n정답:\\t{}\\n예측:\\t{}\\n\\n'.format(\n",
    "        X_bad[i],\n",
    "        y_bad_inversed[i],\n",
    "        y_bad_pred_inversed[i]\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 챗봇 4단계 모델 실행결과 (X_good)\n",
    "for text in X_good:\n",
    "    text_prepared = ' '.join(word for word in okt.morphs(text))\n",
    "    ner = model.analyze(text_prepared)\n",
    "    display(ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 챗봇 4단계 모델 실행결과 (X_bad)\n",
    "for text in X_bad:\n",
    "    text_prepared = ' '.join(word for word in okt.morphs(text))\n",
    "    ner = model.analyze(text_prepared)\n",
    "    display(ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_list = [\n",
    "    '맛있는 제육덮밥 집 알려줘'\n",
    "    ,'파스쿠찌 위치 알려주세요'\n",
    "    ,'담소 순대국밥집 청결한가요?'\n",
    "    ,'담소 순대국밥집 맛있어요?'\n",
    "    ,'담소 순대국밥집 맛있는 곳인가요?'\n",
    "    ,'숯불구이 맛있는곳 알려주실래요?'\n",
    "    ,'온센텐동 사람들 리뷰 좀 알려주세요'\n",
    "    ,'온센텐동 위치가 어떻게 되나요'\n",
    "    ,'파스쿠찌 맛있어요?'\n",
    "    ,'가성비 대박인 육개장 집 알려주세요'\n",
    "    ,'맛있는 육개장집 알려주세요'\n",
    "    ,'육개장 맛있는 곳 알려주세요'\n",
    "    ,'서울에 맛있는 육개장 가게 알려주세요'\n",
    "    ,'제주도에 유명한 흑돼지 식당 알려주세요'\n",
    "    ,'제주도에 맛있는 흑돼지 식당 알려주세요'\n",
    "    ,'제주도에 흑돼지 식당 맛있는 곳 알려주세요'\n",
    "    ,'제주도에 흑돼지 식당 맛집 알려주세요'\n",
    "    ,'제주도에 흑돼지 식당 맛 개쩌는 곳 알려주세요'\n",
    "    ,'맛있고 깔끔한 한식집 아세요'\n",
    "    ,'맛있는 깔끔한 한식집 아세요'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# okt 실행결과\n",
    "for text in q_list:\n",
    "    text_prepared = ' '.join(word for word in okt.morphs(text))\n",
    "    ner = model.analyze(text_prepared)\n",
    "    display(ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mecab 실행결과\n",
    "for text in q_list:\n",
    "    text_prepared = ' '.join(word for word in mecab.morphs(text))\n",
    "    ner = model.analyze(text_prepared)\n",
    "    display(ner)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ner-anago",
   "language": "python",
   "name": "ner-anago"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
